{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projid</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>vid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>device</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch_sec</th>\n",
       "      <th>avg_train_loss</th>\n",
       "      <th>average_valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-16 12:17:15</td>\n",
       "      <td>2d13b533e550c6fdec9b8f345a972c074e9f1123</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.8440802097320557</td>\n",
       "      <td>0.7350246906280518</td>\n",
       "      <td>0.697250135242939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-16 12:17:15</td>\n",
       "      <td>2d13b533e550c6fdec9b8f345a972c074e9f1123</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.8440802097320557</td>\n",
       "      <td>0.7125582695007324</td>\n",
       "      <td>0.6972402011354765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-16 12:17:15</td>\n",
       "      <td>2d13b533e550c6fdec9b8f345a972c074e9f1123</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.8440802097320557</td>\n",
       "      <td>0.7163955271244049</td>\n",
       "      <td>0.6972307165463766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-16 12:17:15</td>\n",
       "      <td>2d13b533e550c6fdec9b8f345a972c074e9f1123</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.8440802097320557</td>\n",
       "      <td>0.7137446999549866</td>\n",
       "      <td>0.6972266410787901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-16 12:17:15</td>\n",
       "      <td>2d13b533e550c6fdec9b8f345a972c074e9f1123</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>cuda:0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.8440802097320557</td>\n",
       "      <td>0.6836660206317902</td>\n",
       "      <td>0.6972261567910513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7725</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-24 14:07:22</td>\n",
       "      <td>3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6943822354078293</td>\n",
       "      <td>0.6821258788307508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7726</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-24 14:07:22</td>\n",
       "      <td>3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5</td>\n",
       "      <td>20</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6907915621995926</td>\n",
       "      <td>0.6822079469760259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7727</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-24 14:07:22</td>\n",
       "      <td>3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5</td>\n",
       "      <td>20</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6827793419361115</td>\n",
       "      <td>0.6818166375160217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-24 14:07:22</td>\n",
       "      <td>3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5</td>\n",
       "      <td>20</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6651844531297684</td>\n",
       "      <td>0.6811445082227389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7729</th>\n",
       "      <td>kaggle-nlp-disasters_flor.shadow</td>\n",
       "      <td>2023-02-24 14:07:22</td>\n",
       "      <td>3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5</td>\n",
       "      <td>20</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6310026198625565</td>\n",
       "      <td>0.6807918548583984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7730 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                projid              tstamp  \\\n",
       "0     kaggle-nlp-disasters_flor.shadow 2023-02-16 12:17:15   \n",
       "1     kaggle-nlp-disasters_flor.shadow 2023-02-16 12:17:15   \n",
       "2     kaggle-nlp-disasters_flor.shadow 2023-02-16 12:17:15   \n",
       "3     kaggle-nlp-disasters_flor.shadow 2023-02-16 12:17:15   \n",
       "4     kaggle-nlp-disasters_flor.shadow 2023-02-16 12:17:15   \n",
       "...                                ...                 ...   \n",
       "7725  kaggle-nlp-disasters_flor.shadow 2023-02-24 14:07:22   \n",
       "7726  kaggle-nlp-disasters_flor.shadow 2023-02-24 14:07:22   \n",
       "7727  kaggle-nlp-disasters_flor.shadow 2023-02-24 14:07:22   \n",
       "7728  kaggle-nlp-disasters_flor.shadow 2023-02-24 14:07:22   \n",
       "7729  kaggle-nlp-disasters_flor.shadow 2023-02-24 14:07:22   \n",
       "\n",
       "                                           vid  epoch  step  \\\n",
       "0     2d13b533e550c6fdec9b8f345a972c074e9f1123      1   4.0   \n",
       "1     2d13b533e550c6fdec9b8f345a972c074e9f1123      1   8.0   \n",
       "2     2d13b533e550c6fdec9b8f345a972c074e9f1123      1  12.0   \n",
       "3     2d13b533e550c6fdec9b8f345a972c074e9f1123      1  16.0   \n",
       "4     2d13b533e550c6fdec9b8f345a972c074e9f1123      1  20.0   \n",
       "...                                        ...    ...   ...   \n",
       "7725  3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5     20  80.0   \n",
       "7726  3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5     20  84.0   \n",
       "7727  3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5     20  88.0   \n",
       "7728  3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5     20  92.0   \n",
       "7729  3fd0cd8855c6e617b1bf43cfdf9cf5639c55bcb5     20  96.0   \n",
       "\n",
       "                          optimizer  device learning_rate           epoch_sec  \\\n",
       "0     <class 'torch.optim.sgd.SGD'>  cuda:0        0.0001  1.8440802097320557   \n",
       "1     <class 'torch.optim.sgd.SGD'>  cuda:0        0.0001  1.8440802097320557   \n",
       "2     <class 'torch.optim.sgd.SGD'>  cuda:0        0.0001  1.8440802097320557   \n",
       "3     <class 'torch.optim.sgd.SGD'>  cuda:0        0.0001  1.8440802097320557   \n",
       "4     <class 'torch.optim.sgd.SGD'>  cuda:0        0.0001  1.8440802097320557   \n",
       "...                             ...     ...           ...                 ...   \n",
       "7725                            NaN     NaN           NaN                 NaN   \n",
       "7726                            NaN     NaN           NaN                 NaN   \n",
       "7727                            NaN     NaN           NaN                 NaN   \n",
       "7728                            NaN     NaN           NaN                 NaN   \n",
       "7729                            NaN     NaN           NaN                 NaN   \n",
       "\n",
       "          avg_train_loss  average_valid_loss  \n",
       "0     0.7350246906280518   0.697250135242939  \n",
       "1     0.7125582695007324  0.6972402011354765  \n",
       "2     0.7163955271244049  0.6972307165463766  \n",
       "3     0.7137446999549866  0.6972266410787901  \n",
       "4     0.6836660206317902  0.6972261567910513  \n",
       "...                  ...                 ...  \n",
       "7725  0.6943822354078293  0.6821258788307508  \n",
       "7726  0.6907915621995926  0.6822079469760259  \n",
       "7727  0.6827793419361115  0.6818166375160217  \n",
       "7728  0.6651844531297684  0.6811445082227389  \n",
       "7729  0.6310026198625565  0.6807918548583984  \n",
       "\n",
       "[7730 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flor\n",
    "full_pivot = flor.full_pivot()\n",
    "full_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rogarcia/anaconda3/envs/kaggle-nlp-dist/lib/python3.9/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
      "Epoch [1/20], LR: 0.000, Step [4/1920], Train Loss: 0.6886, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [8/1920], Train Loss: 0.6897, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [12/1920], Train Loss: 0.7043, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [16/1920], Train Loss: 0.6940, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [20/1920], Train Loss: 0.6991, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [24/1920], Train Loss: 0.7163, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [28/1920], Train Loss: 0.7128, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [32/1920], Train Loss: 0.6821, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [36/1920], Train Loss: 0.7054, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [40/1920], Train Loss: 0.6990, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [44/1920], Train Loss: 0.6968, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [48/1920], Train Loss: 0.7149, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [52/1920], Train Loss: 0.7067, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [56/1920], Train Loss: 0.6992, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [60/1920], Train Loss: 0.7030, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [64/1920], Train Loss: 0.7108, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [68/1920], Train Loss: 0.7123, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [72/1920], Train Loss: 0.6950, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [76/1920], Train Loss: 0.6999, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [80/1920], Train Loss: 0.6935, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [84/1920], Train Loss: 0.6975, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [88/1920], Train Loss: 0.6956, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [92/1920], Train Loss: 0.6945, Valid Loss: 0.6902\n",
      "Epoch [1/20], LR: 0.000, Step [96/1920], Train Loss: 0.6930, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [100/1920], Train Loss: 0.6910, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [104/1920], Train Loss: 0.7045, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [108/1920], Train Loss: 0.6966, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [112/1920], Train Loss: 0.7119, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [116/1920], Train Loss: 0.7081, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [120/1920], Train Loss: 0.6991, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [124/1920], Train Loss: 0.7254, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [128/1920], Train Loss: 0.6940, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [132/1920], Train Loss: 0.6895, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [136/1920], Train Loss: 0.6911, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [140/1920], Train Loss: 0.6983, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [144/1920], Train Loss: 0.6954, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [148/1920], Train Loss: 0.7123, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [152/1920], Train Loss: 0.7179, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [156/1920], Train Loss: 0.7002, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [160/1920], Train Loss: 0.7078, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [164/1920], Train Loss: 0.7044, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [168/1920], Train Loss: 0.7098, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [172/1920], Train Loss: 0.6913, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [176/1920], Train Loss: 0.6919, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [180/1920], Train Loss: 0.7190, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [184/1920], Train Loss: 0.6963, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [188/1920], Train Loss: 0.6688, Valid Loss: 0.6901\n",
      "Epoch [2/20], LR: 0.000, Step [192/1920], Train Loss: 0.6936, Valid Loss: 0.6901\n",
      "Epoch [3/20], LR: 0.000, Step [196/1920], Train Loss: 0.6922, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [200/1920], Train Loss: 0.7072, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [204/1920], Train Loss: 0.6914, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [208/1920], Train Loss: 0.6917, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [212/1920], Train Loss: 0.7046, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [216/1920], Train Loss: 0.6973, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [220/1920], Train Loss: 0.7132, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [224/1920], Train Loss: 0.6960, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [228/1920], Train Loss: 0.7072, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [232/1920], Train Loss: 0.7061, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [236/1920], Train Loss: 0.7060, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [240/1920], Train Loss: 0.7106, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [244/1920], Train Loss: 0.7142, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [248/1920], Train Loss: 0.7016, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [252/1920], Train Loss: 0.6773, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [256/1920], Train Loss: 0.7027, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [260/1920], Train Loss: 0.7013, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [264/1920], Train Loss: 0.7089, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [268/1920], Train Loss: 0.7010, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [272/1920], Train Loss: 0.7004, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [276/1920], Train Loss: 0.6967, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [280/1920], Train Loss: 0.7032, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [284/1920], Train Loss: 0.6984, Valid Loss: 0.6900\n",
      "Epoch [3/20], LR: 0.000, Step [288/1920], Train Loss: 0.6723, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [292/1920], Train Loss: 0.6823, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [296/1920], Train Loss: 0.6863, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [300/1920], Train Loss: 0.6958, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [304/1920], Train Loss: 0.6953, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [308/1920], Train Loss: 0.7011, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [312/1920], Train Loss: 0.7029, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [316/1920], Train Loss: 0.7058, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [320/1920], Train Loss: 0.7136, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [324/1920], Train Loss: 0.6876, Valid Loss: 0.6900\n",
      "Epoch [4/20], LR: 0.000, Step [328/1920], Train Loss: 0.7046, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [332/1920], Train Loss: 0.7041, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [336/1920], Train Loss: 0.6931, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [340/1920], Train Loss: 0.7042, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [344/1920], Train Loss: 0.7126, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [348/1920], Train Loss: 0.7113, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [352/1920], Train Loss: 0.7235, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [356/1920], Train Loss: 0.6949, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [360/1920], Train Loss: 0.7220, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [364/1920], Train Loss: 0.7044, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [368/1920], Train Loss: 0.6995, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [372/1920], Train Loss: 0.7119, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [376/1920], Train Loss: 0.7053, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [380/1920], Train Loss: 0.7031, Valid Loss: 0.6899\n",
      "Epoch [4/20], LR: 0.000, Step [384/1920], Train Loss: 0.6863, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [388/1920], Train Loss: 0.7011, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [392/1920], Train Loss: 0.7041, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [396/1920], Train Loss: 0.6924, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [400/1920], Train Loss: 0.6930, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [404/1920], Train Loss: 0.7114, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [408/1920], Train Loss: 0.7126, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [412/1920], Train Loss: 0.7016, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [416/1920], Train Loss: 0.7037, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [420/1920], Train Loss: 0.6900, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [424/1920], Train Loss: 0.6854, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [428/1920], Train Loss: 0.7063, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [432/1920], Train Loss: 0.7089, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [436/1920], Train Loss: 0.7168, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [440/1920], Train Loss: 0.6986, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [444/1920], Train Loss: 0.6894, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [448/1920], Train Loss: 0.6904, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [452/1920], Train Loss: 0.7032, Valid Loss: 0.6899\n",
      "Epoch [5/20], LR: 0.000, Step [456/1920], Train Loss: 0.7091, Valid Loss: 0.6898\n",
      "Epoch [5/20], LR: 0.000, Step [460/1920], Train Loss: 0.7267, Valid Loss: 0.6898\n",
      "Epoch [5/20], LR: 0.000, Step [464/1920], Train Loss: 0.7069, Valid Loss: 0.6898\n",
      "Epoch [5/20], LR: 0.000, Step [468/1920], Train Loss: 0.7021, Valid Loss: 0.6898\n",
      "Epoch [5/20], LR: 0.000, Step [472/1920], Train Loss: 0.6883, Valid Loss: 0.6898\n",
      "Epoch [5/20], LR: 0.000, Step [476/1920], Train Loss: 0.6886, Valid Loss: 0.6898\n",
      "Epoch [5/20], LR: 0.000, Step [480/1920], Train Loss: 0.6850, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [484/1920], Train Loss: 0.6775, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [488/1920], Train Loss: 0.6918, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [492/1920], Train Loss: 0.7072, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [496/1920], Train Loss: 0.7097, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [500/1920], Train Loss: 0.6959, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [504/1920], Train Loss: 0.7055, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [508/1920], Train Loss: 0.7087, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [512/1920], Train Loss: 0.6886, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [516/1920], Train Loss: 0.6941, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [520/1920], Train Loss: 0.6885, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [524/1920], Train Loss: 0.7103, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [528/1920], Train Loss: 0.6807, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [532/1920], Train Loss: 0.7155, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [536/1920], Train Loss: 0.7124, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [540/1920], Train Loss: 0.7201, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [544/1920], Train Loss: 0.6984, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [548/1920], Train Loss: 0.7387, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [552/1920], Train Loss: 0.6955, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [556/1920], Train Loss: 0.7070, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [560/1920], Train Loss: 0.7048, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [564/1920], Train Loss: 0.7005, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [568/1920], Train Loss: 0.7069, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [572/1920], Train Loss: 0.6892, Valid Loss: 0.6898\n",
      "Epoch [6/20], LR: 0.000, Step [576/1920], Train Loss: 0.6959, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [580/1920], Train Loss: 0.7123, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [584/1920], Train Loss: 0.7083, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [588/1920], Train Loss: 0.6940, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [592/1920], Train Loss: 0.6997, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [596/1920], Train Loss: 0.6930, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [600/1920], Train Loss: 0.7011, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [604/1920], Train Loss: 0.7009, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [608/1920], Train Loss: 0.6872, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [612/1920], Train Loss: 0.6866, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [616/1920], Train Loss: 0.6879, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [620/1920], Train Loss: 0.7055, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [624/1920], Train Loss: 0.6961, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [628/1920], Train Loss: 0.6972, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [632/1920], Train Loss: 0.6986, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [636/1920], Train Loss: 0.6998, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [640/1920], Train Loss: 0.7064, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [644/1920], Train Loss: 0.7098, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [648/1920], Train Loss: 0.6944, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [652/1920], Train Loss: 0.7021, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [656/1920], Train Loss: 0.7011, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [660/1920], Train Loss: 0.6913, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [664/1920], Train Loss: 0.6939, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [668/1920], Train Loss: 0.6934, Valid Loss: 0.6897\n",
      "Epoch [7/20], LR: 0.000, Step [672/1920], Train Loss: 0.6718, Valid Loss: 0.6897\n",
      "Epoch [8/20], LR: 0.000, Step [676/1920], Train Loss: 0.6848, Valid Loss: 0.6897\n",
      "Epoch [8/20], LR: 0.000, Step [680/1920], Train Loss: 0.7004, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [684/1920], Train Loss: 0.7072, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [688/1920], Train Loss: 0.6950, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [692/1920], Train Loss: 0.7033, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [696/1920], Train Loss: 0.7024, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [700/1920], Train Loss: 0.6903, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [704/1920], Train Loss: 0.7069, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [708/1920], Train Loss: 0.6961, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [712/1920], Train Loss: 0.6894, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [716/1920], Train Loss: 0.7091, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [720/1920], Train Loss: 0.7013, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [724/1920], Train Loss: 0.7043, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [728/1920], Train Loss: 0.7057, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [732/1920], Train Loss: 0.6920, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [736/1920], Train Loss: 0.7129, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [740/1920], Train Loss: 0.7002, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [744/1920], Train Loss: 0.7006, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [748/1920], Train Loss: 0.6922, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [752/1920], Train Loss: 0.6912, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [756/1920], Train Loss: 0.7025, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [760/1920], Train Loss: 0.7053, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [764/1920], Train Loss: 0.7118, Valid Loss: 0.6896\n",
      "Epoch [8/20], LR: 0.000, Step [768/1920], Train Loss: 0.7013, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [772/1920], Train Loss: 0.6866, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [776/1920], Train Loss: 0.7093, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [780/1920], Train Loss: 0.6900, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [784/1920], Train Loss: 0.7066, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [788/1920], Train Loss: 0.6947, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [792/1920], Train Loss: 0.7071, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [796/1920], Train Loss: 0.6963, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [800/1920], Train Loss: 0.7056, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [804/1920], Train Loss: 0.7074, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [808/1920], Train Loss: 0.7060, Valid Loss: 0.6896\n",
      "Epoch [9/20], LR: 0.000, Step [812/1920], Train Loss: 0.6900, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [816/1920], Train Loss: 0.6936, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [820/1920], Train Loss: 0.7087, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [824/1920], Train Loss: 0.7129, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [828/1920], Train Loss: 0.7021, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [832/1920], Train Loss: 0.7055, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [836/1920], Train Loss: 0.6999, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [840/1920], Train Loss: 0.7134, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [844/1920], Train Loss: 0.7031, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [848/1920], Train Loss: 0.7064, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [852/1920], Train Loss: 0.6976, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [856/1920], Train Loss: 0.7086, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [860/1920], Train Loss: 0.6933, Valid Loss: 0.6895\n",
      "Epoch [9/20], LR: 0.000, Step [864/1920], Train Loss: 0.6807, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [868/1920], Train Loss: 0.6878, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [872/1920], Train Loss: 0.7017, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [876/1920], Train Loss: 0.7015, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [880/1920], Train Loss: 0.7054, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [884/1920], Train Loss: 0.7136, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [888/1920], Train Loss: 0.6990, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [892/1920], Train Loss: 0.6966, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [896/1920], Train Loss: 0.7236, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [900/1920], Train Loss: 0.6914, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [904/1920], Train Loss: 0.6867, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [908/1920], Train Loss: 0.7038, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [912/1920], Train Loss: 0.6979, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [916/1920], Train Loss: 0.6986, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [920/1920], Train Loss: 0.6941, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [924/1920], Train Loss: 0.7050, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [928/1920], Train Loss: 0.7093, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [932/1920], Train Loss: 0.6846, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [936/1920], Train Loss: 0.7049, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [940/1920], Train Loss: 0.7007, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [944/1920], Train Loss: 0.6928, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [948/1920], Train Loss: 0.7053, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [952/1920], Train Loss: 0.7016, Valid Loss: 0.6895\n",
      "Epoch [10/20], LR: 0.000, Step [956/1920], Train Loss: 0.6981, Valid Loss: 0.6894\n",
      "Epoch [10/20], LR: 0.000, Step [960/1920], Train Loss: 0.6696, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [964/1920], Train Loss: 0.6819, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [968/1920], Train Loss: 0.7052, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [972/1920], Train Loss: 0.6903, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [976/1920], Train Loss: 0.7039, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [980/1920], Train Loss: 0.7008, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [984/1920], Train Loss: 0.6973, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [988/1920], Train Loss: 0.6760, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [992/1920], Train Loss: 0.6963, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [996/1920], Train Loss: 0.7209, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1000/1920], Train Loss: 0.6959, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1004/1920], Train Loss: 0.6910, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1008/1920], Train Loss: 0.7033, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1012/1920], Train Loss: 0.6987, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1016/1920], Train Loss: 0.7071, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1020/1920], Train Loss: 0.7031, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1024/1920], Train Loss: 0.7228, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1028/1920], Train Loss: 0.6895, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1032/1920], Train Loss: 0.7053, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1036/1920], Train Loss: 0.7015, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1040/1920], Train Loss: 0.7069, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1044/1920], Train Loss: 0.7247, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1048/1920], Train Loss: 0.6946, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1052/1920], Train Loss: 0.6926, Valid Loss: 0.6894\n",
      "Epoch [11/20], LR: 0.000, Step [1056/1920], Train Loss: 0.6953, Valid Loss: 0.6894\n",
      "Epoch [12/20], LR: 0.000, Step [1060/1920], Train Loss: 0.6754, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1064/1920], Train Loss: 0.6797, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1068/1920], Train Loss: 0.6902, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1072/1920], Train Loss: 0.6975, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1076/1920], Train Loss: 0.6977, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1080/1920], Train Loss: 0.6953, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1084/1920], Train Loss: 0.6911, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1088/1920], Train Loss: 0.7188, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1092/1920], Train Loss: 0.7016, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1096/1920], Train Loss: 0.6852, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1100/1920], Train Loss: 0.7024, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1104/1920], Train Loss: 0.7053, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1108/1920], Train Loss: 0.6930, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1112/1920], Train Loss: 0.7119, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1116/1920], Train Loss: 0.7040, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1120/1920], Train Loss: 0.6980, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1124/1920], Train Loss: 0.6912, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1128/1920], Train Loss: 0.7056, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1132/1920], Train Loss: 0.6908, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1136/1920], Train Loss: 0.7003, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1140/1920], Train Loss: 0.7021, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1144/1920], Train Loss: 0.7107, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1148/1920], Train Loss: 0.6858, Valid Loss: 0.6893\n",
      "Epoch [12/20], LR: 0.000, Step [1152/1920], Train Loss: 0.6784, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1156/1920], Train Loss: 0.6838, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1160/1920], Train Loss: 0.7040, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1164/1920], Train Loss: 0.6908, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1168/1920], Train Loss: 0.6999, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1172/1920], Train Loss: 0.7050, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1176/1920], Train Loss: 0.6845, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1180/1920], Train Loss: 0.6897, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1184/1920], Train Loss: 0.7184, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1188/1920], Train Loss: 0.6915, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1192/1920], Train Loss: 0.7027, Valid Loss: 0.6893\n",
      "Epoch [13/20], LR: 0.000, Step [1196/1920], Train Loss: 0.6926, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1200/1920], Train Loss: 0.6914, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1204/1920], Train Loss: 0.6957, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1208/1920], Train Loss: 0.6873, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1212/1920], Train Loss: 0.6974, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1216/1920], Train Loss: 0.7115, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1220/1920], Train Loss: 0.7011, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1224/1920], Train Loss: 0.7041, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1228/1920], Train Loss: 0.6993, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1232/1920], Train Loss: 0.7005, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1236/1920], Train Loss: 0.7058, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1240/1920], Train Loss: 0.6832, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1244/1920], Train Loss: 0.6993, Valid Loss: 0.6892\n",
      "Epoch [13/20], LR: 0.000, Step [1248/1920], Train Loss: 0.6972, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1252/1920], Train Loss: 0.6811, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1256/1920], Train Loss: 0.7059, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1260/1920], Train Loss: 0.6896, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1264/1920], Train Loss: 0.7032, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1268/1920], Train Loss: 0.6871, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1272/1920], Train Loss: 0.6875, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1276/1920], Train Loss: 0.6878, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1280/1920], Train Loss: 0.6917, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1284/1920], Train Loss: 0.6921, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1288/1920], Train Loss: 0.7084, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1292/1920], Train Loss: 0.7007, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1296/1920], Train Loss: 0.7114, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1300/1920], Train Loss: 0.7027, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1304/1920], Train Loss: 0.6982, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1308/1920], Train Loss: 0.6992, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1312/1920], Train Loss: 0.7020, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1316/1920], Train Loss: 0.7069, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1320/1920], Train Loss: 0.7062, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1324/1920], Train Loss: 0.7106, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1328/1920], Train Loss: 0.6964, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1332/1920], Train Loss: 0.7146, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1336/1920], Train Loss: 0.7087, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1340/1920], Train Loss: 0.6728, Valid Loss: 0.6892\n",
      "Epoch [14/20], LR: 0.000, Step [1344/1920], Train Loss: 0.7061, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1348/1920], Train Loss: 0.6810, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1352/1920], Train Loss: 0.7006, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1356/1920], Train Loss: 0.7092, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1360/1920], Train Loss: 0.6876, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1364/1920], Train Loss: 0.6908, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1368/1920], Train Loss: 0.7159, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1372/1920], Train Loss: 0.7128, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1376/1920], Train Loss: 0.6942, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1380/1920], Train Loss: 0.6960, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1384/1920], Train Loss: 0.7094, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1388/1920], Train Loss: 0.7216, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1392/1920], Train Loss: 0.7069, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1396/1920], Train Loss: 0.6990, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1400/1920], Train Loss: 0.7021, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1404/1920], Train Loss: 0.7144, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1408/1920], Train Loss: 0.7016, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1412/1920], Train Loss: 0.6912, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1416/1920], Train Loss: 0.7217, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1420/1920], Train Loss: 0.7107, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1424/1920], Train Loss: 0.6982, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1428/1920], Train Loss: 0.7005, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1432/1920], Train Loss: 0.6915, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1436/1920], Train Loss: 0.7004, Valid Loss: 0.6891\n",
      "Epoch [15/20], LR: 0.000, Step [1440/1920], Train Loss: 0.7092, Valid Loss: 0.6891\n",
      "Epoch [16/20], LR: 0.000, Step [1444/1920], Train Loss: 0.6875, Valid Loss: 0.6891\n",
      "Epoch [16/20], LR: 0.000, Step [1448/1920], Train Loss: 0.6836, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1452/1920], Train Loss: 0.7069, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1456/1920], Train Loss: 0.6910, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1460/1920], Train Loss: 0.7023, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1464/1920], Train Loss: 0.6938, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1468/1920], Train Loss: 0.7062, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1472/1920], Train Loss: 0.7045, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1476/1920], Train Loss: 0.6892, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1480/1920], Train Loss: 0.6966, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1484/1920], Train Loss: 0.6958, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1488/1920], Train Loss: 0.6954, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1492/1920], Train Loss: 0.6941, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1496/1920], Train Loss: 0.7004, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1500/1920], Train Loss: 0.7022, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1504/1920], Train Loss: 0.7186, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1508/1920], Train Loss: 0.7091, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1512/1920], Train Loss: 0.6715, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1516/1920], Train Loss: 0.6939, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1520/1920], Train Loss: 0.6944, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1524/1920], Train Loss: 0.6917, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1528/1920], Train Loss: 0.7010, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1532/1920], Train Loss: 0.6944, Valid Loss: 0.6890\n",
      "Epoch [16/20], LR: 0.000, Step [1536/1920], Train Loss: 0.6907, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1540/1920], Train Loss: 0.6945, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1544/1920], Train Loss: 0.6770, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1548/1920], Train Loss: 0.7026, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1552/1920], Train Loss: 0.6994, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1556/1920], Train Loss: 0.7238, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1560/1920], Train Loss: 0.7070, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1564/1920], Train Loss: 0.7053, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1568/1920], Train Loss: 0.6956, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1572/1920], Train Loss: 0.7027, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1576/1920], Train Loss: 0.6921, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1580/1920], Train Loss: 0.7067, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1584/1920], Train Loss: 0.7056, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1588/1920], Train Loss: 0.6971, Valid Loss: 0.6890\n",
      "Epoch [17/20], LR: 0.000, Step [1592/1920], Train Loss: 0.6900, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1596/1920], Train Loss: 0.7022, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1600/1920], Train Loss: 0.6983, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1604/1920], Train Loss: 0.7052, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1608/1920], Train Loss: 0.6897, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1612/1920], Train Loss: 0.7121, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1616/1920], Train Loss: 0.6986, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1620/1920], Train Loss: 0.7101, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1624/1920], Train Loss: 0.7001, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1628/1920], Train Loss: 0.7018, Valid Loss: 0.6889\n",
      "Epoch [17/20], LR: 0.000, Step [1632/1920], Train Loss: 0.6781, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1636/1920], Train Loss: 0.7036, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1640/1920], Train Loss: 0.6949, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1644/1920], Train Loss: 0.6976, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1648/1920], Train Loss: 0.6892, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1652/1920], Train Loss: 0.6992, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1656/1920], Train Loss: 0.7090, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1660/1920], Train Loss: 0.6887, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1664/1920], Train Loss: 0.6979, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1668/1920], Train Loss: 0.6957, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1672/1920], Train Loss: 0.6969, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1676/1920], Train Loss: 0.6837, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1680/1920], Train Loss: 0.6993, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1684/1920], Train Loss: 0.7000, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1688/1920], Train Loss: 0.6985, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1692/1920], Train Loss: 0.6980, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1696/1920], Train Loss: 0.6838, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1700/1920], Train Loss: 0.7094, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1704/1920], Train Loss: 0.7053, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1708/1920], Train Loss: 0.7039, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1712/1920], Train Loss: 0.6982, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1716/1920], Train Loss: 0.6987, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1720/1920], Train Loss: 0.6960, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1724/1920], Train Loss: 0.6981, Valid Loss: 0.6889\n",
      "Epoch [18/20], LR: 0.000, Step [1728/1920], Train Loss: 0.6682, Valid Loss: 0.6889\n",
      "Epoch [19/20], LR: 0.000, Step [1732/1920], Train Loss: 0.7091, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1736/1920], Train Loss: 0.7058, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1740/1920], Train Loss: 0.6893, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1744/1920], Train Loss: 0.7099, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1748/1920], Train Loss: 0.7072, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1752/1920], Train Loss: 0.6861, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1756/1920], Train Loss: 0.7030, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1760/1920], Train Loss: 0.7167, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1764/1920], Train Loss: 0.6953, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1768/1920], Train Loss: 0.6961, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1772/1920], Train Loss: 0.6978, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1776/1920], Train Loss: 0.6991, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1780/1920], Train Loss: 0.7105, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1784/1920], Train Loss: 0.6989, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1788/1920], Train Loss: 0.6936, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1792/1920], Train Loss: 0.7016, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1796/1920], Train Loss: 0.6998, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1800/1920], Train Loss: 0.7082, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1804/1920], Train Loss: 0.7130, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1808/1920], Train Loss: 0.6864, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1812/1920], Train Loss: 0.6949, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1816/1920], Train Loss: 0.7040, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1820/1920], Train Loss: 0.6996, Valid Loss: 0.6888\n",
      "Epoch [19/20], LR: 0.000, Step [1824/1920], Train Loss: 0.6868, Valid Loss: 0.6888\n",
      "Epoch [20/20], LR: 0.000, Step [1828/1920], Train Loss: 0.6835, Valid Loss: 0.6888\n",
      "Epoch [20/20], LR: 0.000, Step [1832/1920], Train Loss: 0.6991, Valid Loss: 0.6888\n",
      "Epoch [20/20], LR: 0.000, Step [1836/1920], Train Loss: 0.6957, Valid Loss: 0.6888\n",
      "Epoch [20/20], LR: 0.000, Step [1840/1920], Train Loss: 0.6903, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1844/1920], Train Loss: 0.7056, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1848/1920], Train Loss: 0.6965, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1852/1920], Train Loss: 0.6854, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1856/1920], Train Loss: 0.7131, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1860/1920], Train Loss: 0.7122, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1864/1920], Train Loss: 0.6949, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1868/1920], Train Loss: 0.6946, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1872/1920], Train Loss: 0.6928, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1876/1920], Train Loss: 0.6975, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1880/1920], Train Loss: 0.7048, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1884/1920], Train Loss: 0.6918, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1888/1920], Train Loss: 0.7071, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1892/1920], Train Loss: 0.6886, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1896/1920], Train Loss: 0.6965, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1900/1920], Train Loss: 0.7057, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1904/1920], Train Loss: 0.6841, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1908/1920], Train Loss: 0.7089, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1912/1920], Train Loss: 0.7042, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1916/1920], Train Loss: 0.7019, Valid Loss: 0.6887\n",
      "Epoch [20/20], LR: 0.000, Step [1920/1920], Train Loss: 0.6959, Valid Loss: 0.6887\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "!python train_rnn.py --flor constantlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flor\n",
    "facts = flor.log_records() \n",
    "# Calling it for the unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['projid', 'tstamp', 'vid', 'epoch', 'step', 'optimizer', 'device',\n",
       "       'learning_rate', 'epoch_sec', 'avg_train_loss', 'average_valid_loss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pivot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pivot[full_pivot['optimizer'].isna()][['tstamp', 'vid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pivot.to_csv('~/Desktop/fullpivot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout 8eaf80555161e95bbdec75f1ec6324dd1fc3febb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flor.apply(['device', 'optimizer'], 'train_rnn.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-nlp-dist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815c30a49f552e6fdb87f74e117ecdf18b0ca5a01ddc5c83796985c2fbc2bb40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
